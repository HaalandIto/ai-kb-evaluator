import os
for key in (
    "http_proxy", "https_proxy", "all_proxy",
    "HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY"
):
    os.environ.pop(key, None)

import requests                        
session = requests.Session()
session.trust_env = False               
import json
import re
import time
import logging
from logging.handlers import RotatingFileHandler
import pandas as pd
import argparse


from concurrent.futures import ThreadPoolExecutor, as_completed
parser = argparse.ArgumentParser(description="Dify QA & æœ¬åœ°åˆå¹¶å·¥å…·")
parser.add_argument("--kb-name", default="å…±é’å›¢",
                    help="çŸ¥è¯†åº“åç§°å‰ç¼€ï¼Œé»˜è®¤ 'çŸ¥è¯†åº“001'")
parser.add_argument("--max-workers", type=int, default=1,
                    help="æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°")
args = parser.parse_args()

KB_NAME = args.kb_name
MAX_WORKERS = args.max_workers


# ===== åŸºç¡€é…ç½® =====
DATASET_ID       = "4b5259b0-9ea4-4d2d-834a-b8bc4b486a4c"
DATASET_API_KEY  = "Bearer dataset-iP4pEJdWfH1dULoGarO28XLZ"
DATASET_API_BASE = "http://211.90.218.228:8888/v1"

CHAT_API_URL     = "http://211.90.218.228:8888/v1/chat-messages"
CHAT_API_KEY     = "Bearer app-3eIp7a3i1lIGRpDoByO1H22T"
AGENT_ID = "ZXH7u1xHgSmlgFzs"

DEESEEK_API_KEY  = "sk-fbd59078f53246cbaf7a2f7422c872e6"
DEESEEK_API_BASE = "https://api.deepseek.com/v1"

REPORT_DIR       = r"C:\Users\ASUS\Desktop\test654321"
os.makedirs(REPORT_DIR, exist_ok=True)

# ===== æ—¥å¿—é…ç½® =====
LOG_PATH = os.path.join(REPORT_DIR, f"{KB_NAME}_dify_test.log")
logger = logging.getLogger("dify_test")
logger.setLevel(logging.DEBUG)
file_handler = RotatingFileHandler(LOG_PATH, maxBytes=5*1024*1024, backupCount=2, encoding="utf-8")
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
console = logging.StreamHandler()
console.setFormatter(formatter)
logger.addHandler(console)

# ===== å‡½æ•° =====

def sanitize_filename(name: str) -> str:
    safe = "".join(c for c in name if c.isalnum() or c in " -_ï¼ˆï¼‰[]()")
    return f"{KB_NAME}_{(safe[:200] or 'report')}.txt"

def get_documents(dataset_id, retries=2):
    url = f"{DATASET_API_BASE}/datasets/{dataset_id}/documents"
    for attempt in range(retries):
        try:
            r = session.get(url,
                            headers={"Authorization": DATASET_API_KEY},
                            timeout=10,
                            proxies={})
            r.raise_for_status()
            return r.json()
        except Exception as e:
            logger.error(f"get_documents å¤±è´¥: {e}")
            time.sleep(2)
    raise RuntimeError("æ— æ³•è·å–æ–‡æ¡£åˆ—è¡¨")

def parse_file_info(inputs):
    if isinstance(inputs, str):
        inputs = json.loads(inputs)
    text = "\n".join(
        f"{item['id']}: {item['data_source_detail_dict']['upload_file']['name']}"
        for item in inputs.get("data", [])
    )
    matches = re.findall(r"([0-9a-f\-]{36}):\s*([^\n]+?)(?=(?:\s[0-9a-f\-]{36}:)|$)", text)
    return [m[0] for m in matches], [m[1] for m in matches]

def get_segments(dataset_id, file_id, retries=2):
    url = f"{DATASET_API_BASE}/datasets/{dataset_id}/documents/{file_id}/segments"
    params = {"page":1, "page_size":100}
    for attempt in range(retries):
        try:
            r = session.get(url, headers={"Authorization": DATASET_API_KEY}, params=params, timeout=20)
            r.raise_for_status()
            data = r.json().get("data", [])
            segs = data.get("items") if isinstance(data, dict) and "items" in data else data
            if segs:
                logger.info(f"æ–‡æ¡£ {file_id} æ‹¿åˆ° {len(segs)} ä¸ª segment")
                return segs
        except Exception as e:
            logger.error(f"get_segments å¤±è´¥: {e}")
        time.sleep(2)
    logger.warning(f"æ–‡æ¡£ {file_id} segments ä¸ºç©º")
    return []

POSSIBLE_TEXT_KEYS = ["text","content","segment_content","parsed_text","raw_text","body"]
def extract_segment_text(seg):
    for k in POSSIBLE_TEXT_KEYS:
        v = seg.get(k)
        if isinstance(v, str) and v.strip():
            return v.strip()
    return ""

QA_BLOCK_RE = re.compile(r"é—®é¢˜\s*(\d+)[ï¼š:]\s*(.*?)\nç­”æ¡ˆ\s*\1[ï¼š:]\s*(.*?)(?:\n---|\Z)", re.DOTALL)
def parse_plain_qa(raw):
    qa = []
    for m in QA_BLOCK_RE.finditer(raw):
        q, a = m.group(2).strip(), m.group(3).strip()
        if q and a:
            qa.append({"question": q, "answer": a})
    return qa

# ===== LLM ç”Ÿæˆä¸å¯¹æ¯” =====
def generate_qa_pairs_via_llm(text, retries=2):
    if not text.strip():
        return []

    snippet = text[:8000]
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé—®ç­”æç‚¼ä¸“å®¶ï¼Œæ“…é•¿ä»æ­£å¼æ–‡æ¡£ä¸­æå–ç²¾ç¡®è¦ç‚¹ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„æ¸…æ™°çš„é—®é¢˜ä¸æ ‡å‡†ç­”æ¡ˆã€‚\n"
        "è¯·æ ¹æ®ä¸‹æ–¹æ­£å¼æ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«å…¶å…³é”®ä¿¡æ¯ï¼Œæç‚¼å‡º 10 ä¸ªç²¾å‡†ã€ä»£è¡¨æ€§çš„é—®é¢˜åŠå¯¹åº”ç­”æ¡ˆã€‚\n\n"
        "ã€ä»»åŠ¡è¦æ±‚ã€‘\n"
        "1. æ‰€æœ‰é—®é¢˜å¿…é¡»ä¸¥æ ¼ä¾æ®åŸå§‹æ–‡æœ¬ï¼Œä¸å¾—ç¼–é€ ã€å¼•ç”³æˆ–æ³›åŒ–ï¼Œå¹¶ä¸”å°½é‡æ¨¡ä»¿è€å¸ˆå’Œå­¦ç”Ÿå¯èƒ½æå‡ºæ¥çš„é—®é¢˜ï¼›\n"
        "2. é—®é¢˜å¿…é¡»æ˜ç¡®ã€å®Œæ•´ï¼Œå¯ç‹¬ç«‹ç†è§£ï¼Œä¸å¾—åŒ…å«ä»¥ä¸‹å†…å®¹æˆ–ç‰¹å¾ï¼š\n"
        "   - æ¨¡ç³ŠæŒ‡ä»£ï¼Œå¦‚â€œæ–‡ä»¶ç¼–å·â€â€œæ–‡å·â€â€œæ–‡ä»¶åç§°â€â€œæœ¬æ–‡ä»¶â€â€œæœ¬éƒ¨é—¨â€â€œæœ¬åˆ¶åº¦â€â€œè¯¥æ¡æ¬¾â€â€œä¸Šè¿°è§„å®šâ€â€œä»¥ä¸Šå†…å®¹â€â€œç›¸å…³äº‹é¡¹â€â€œè¿™äº›æ–‡ä»¶â€ç­‰ï¼›\n"
        "   - éœ€è¦ä¾èµ–ä¸Šä¸‹æ–‡æ‰èƒ½ç†è§£çš„é—®é¢˜ï¼Œæˆ–é—®é¢˜æè¿°ä¸è‡ªæ´½ï¼›\n"
        "   - å¸¦æœ‰æ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ç­‰ä¸æ˜ç¡®æŒ‡å‘çš„æ¨¡ç³Šè¡¨è¾¾ï¼Œå¦‚â€œè¿‘æœŸâ€â€œæœ‰å…³äººå‘˜â€â€œç›¸å…³éƒ¨é—¨â€ç­‰ï¼›\n"
        "   - å¸¦æœ‰å¤šé‡æ­§ä¹‰æˆ–å¯è‡ªç”±è§£é‡Šç©ºé—´çš„é—®é¢˜ï¼›\n"
        "   - ä½¿ç”¨â€œæ˜¯å¦â€â€œå¯ä»¥å—â€ç­‰ç®€å•æ˜¯éé—®æ³•ï¼Œä½†ç¼ºå°‘å®Œæ•´æƒ…å¢ƒçš„é—®é¢˜ã€‚\n"
        "   - ä¸å¾—é—®æ–‡ä»¶æˆ–è€…åŠæ³•çš„å®æ–½æ—¥æœŸ,ä¿®è®¢æŸæŸåˆ¶åº¦çš„ç›®çš„ ä»¥åŠå°å‘æ—¥æœŸè¿™äº›ä¸»è§‚æ€§å¼ºæˆ–è€…æ— æ„ä¹‰çš„é—®é¢˜ã€‚ \n"
        "3. é—®é¢˜åº”èšç„¦äºï¼šåˆ¶åº¦æ¡æ¬¾ã€å®šé‡æŒ‡æ ‡ã€ç§¯åˆ†æ ‡å‡†ã€æ“ä½œæµç¨‹ã€è§’è‰²èŒè´£ã€ä½¿ç”¨æ¡ä»¶ç­‰ï¼›\n"
        "4. ç­”æ¡ˆå¿…é¡»ï¼š\n"
        "   - ç´§æ‰£é¢˜å¹²æé—®è¦ç‚¹ï¼Œå…¨é¢è¦†ç›–åº”ç­”å†…å®¹ï¼›\n"
        "   - å°½å¯èƒ½ä½¿ç”¨åŸæ–‡æœ¯è¯­å’Œæªè¾ï¼Œé¿å…åŒä¹‰æ”¹å†™ï¼›\n"
        "   - æ¶‰åŠå¤šé¡¹å†…å®¹æ—¶ï¼Œä½¿ç”¨åºå·ã€ç ´æŠ˜å·æˆ–åˆ—è¡¨åˆ†ç‚¹è¡¨è¾¾ï¼›\n"
        "   - æ‰€æœ‰æ•°å€¼ã€åè¯ã€é™å®šæ¡ä»¶å¿…é¡»ä¸åŸæ–‡ä¿æŒä¸€è‡´ï¼›\n"
        "5. ä¸¥ç¦ç”Ÿæˆä»¥ä¸‹å†…å®¹ï¼š\n"
        "   - æ–‡æœ¬æ€»ç»“æˆ–æ¦‚è¿°ç±»å†…å®¹ï¼›\n"
        "   - æ¨ç†æ€§è§£é‡Šã€è¯„ä»·æ€§è¯­è¨€æˆ–è·¨æ®µæ•´åˆï¼›\n"
        "   - æ¨¡ç³Šã€å®½æ³›ã€ä¸èšç„¦çš„é—®é¢˜ï¼›\n"
        "6. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼ä¸º JSON æ•°ç»„ï¼Œä¸”æ»¡è¶³ä»¥ä¸‹ç»“æ„ï¼ˆæ³¨æ„å­—æ®µå¤§å°å†™ï¼‰ï¼š\n"
        "[\n"
        '  { "Q": "é—®é¢˜1ï¼Ÿ", "A": "ç­”æ¡ˆ1ã€‚" },\n'
        '  { "Q": "é—®é¢˜2ï¼Ÿ", "A": "ç­”æ¡ˆ2ã€‚" },\n'
        '  { "Q": "é—®é¢˜3ï¼Ÿ", "A": "ç­”æ¡ˆ3ã€‚" }\n'
        "]\n"
        "7. å¿…é¡»ä»¥ [ å¼€å¤´ã€ä»¥ ] ç»“å°¾ï¼Œç¦æ­¢æ·»åŠ  markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰æˆ–è§£é‡Šè¯´æ˜ã€‚\n"
        "8. ä»…è¾“å‡º JSON å†…å®¹ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–æç¤ºã€æ³¨é‡Šã€æ¢è¡Œç¬¦ä»¥å¤–çš„æ–‡æœ¬ã€‚\n\n"
        "ã€è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šæ ¼å¼ï¼Œä»…æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆé—®ç­”ã€‘ï¼š\n"
        f"{snippet}"
    )


    for _ in range(retries):
        try:
            r = session.post(
                f"{DEESEEK_API_BASE}/chat/completions",
                headers={
                    "Authorization": f"Bearer {DEESEEK_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0
                },
                timeout=90
            )
            r.raise_for_status()
            raw = r.json()["choices"][0]["message"]["content"]

            m = re.search(r"\[\s*{.*?}\s*\]", raw, re.DOTALL)
            if m:
                arr = json.loads(m.group(0))
                qa = [
                    {
                        "question": o.get("Q") or o.get("question", ""),
                        "answer": o.get("A") or o.get("answer", "")
                    }
                    for o in arr
                    if o.get("Q") or o.get("question")
                ]
                if qa:
                    return qa[:10]

            qa_plain = parse_plain_qa(raw)
            if qa_plain:
                return qa_plain[:10]

        except Exception as e:
            logger.error(f"generate_qa_pairs_via_llm å¤±è´¥: {e}")
            time.sleep(2)

    return []


def compare_qa(qa_list, answers, contexts, retries=3):
    def _compare_one(idx, qa, a_t, context_text):
        question_text = qa["question"]
        answer_x_text = qa["answer"]
        answer_y_text = a_t
        prompt = (
            f"ä½ å°†æ”¶åˆ°å¤šä¸ªé—®é¢˜çš„ä¸¤ä¸ªç‰ˆæœ¬ç­”æ¡ˆï¼ˆå¦‚ï¼šç­”æ¡ˆ X-{idx} å’Œ ç­”æ¡ˆ Y-{idx}ï¼‰ï¼Œå®ƒä»¬æ˜¯å¯¹åŒä¸€ä¸ªé—®é¢˜çš„å›ç­”ã€‚\n"
           "ä½ çš„ä»»åŠ¡æ˜¯ï¼š\n"
            f"  é€å¯¹å¯¹æ¯” X-{idx} ä¸ Y-{idx}ï¼›\n"
            f"  ä»¥ X-{idx} ä¸ºæ ‡å‡†ï¼Œåˆ¤æ–­ Y-{idx} æ˜¯å¦å‡†ç¡®è¡¨è¾¾äº†æ ¸å¿ƒå†…å®¹ï¼›\n"
            f"  å¦‚æœ Y-{idx} æœ‰é—æ¼ã€ç†è§£é”™è¯¯æˆ–æ‰©å±•ä¸å½“ï¼Œè¯·åœ¨â€œç†ç”±â€ä¸­æŒ‡å‡ºï¼›\n"
            "  å¯¹äºè¯­åºã€è¿‘ä¹‰è¯æˆ–æ ‡ç‚¹å·®å¼‚ä¸æ‰£åˆ†ï¼›\n"
            "  ç»™å‡ºç›¸ä¼¼åº¦ï¼ˆ0â€“100ï¼‰å’Œè¯„çº§ï¼ˆä¼˜ç§€ / åˆæ ¼ / ä¸åˆæ ¼ï¼‰ã€‚\n"
            "è¯„åˆ†æ ‡å‡†ï¼ˆæ»¡è¶³æ­£æ€åˆ†å¸ƒï¼Œä¼˜ç§€ï¼šåˆæ ¼ï¼šä¸åˆæ ¼ â‰ˆ 1ï¼š8ï¼š1ï¼‰ï¼š\n"
            "  ç›¸ä¼¼åº¦ > 90ï¼šä¼˜ç§€ï¼ˆçº¦ 10%ï¼‰\n"
            "  ç›¸ä¼¼åº¦ 60â€“90ï¼šåˆæ ¼ï¼ˆçº¦ 80%ï¼‰\n"
            "  ç›¸ä¼¼åº¦ < 60ï¼šä¸åˆæ ¼ï¼ˆçº¦ 10%ï¼‰\n"
            "è¯´æ˜ï¼šåˆ†æ•°å°½é‡ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œé¿å…é›†ä¸­åœ¨æç«¯å€¼ã€‚\n"
            "è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š\n"
            "é¢˜å·ï¼š\n"
            "é¢˜å¹²ï¼š\n"
            f"ç­”æ¡ˆ X-{idx}ï¼š\n"
            f"ç­”æ¡ˆ Y-{idx}ï¼š\n"
            "ç†ç”±ï¼š\n"
            "ç›¸ä¼¼åº¦ï¼š\n"
            "è¯„çº§ï¼š\n"
            "æ¯é¢˜ä¹‹é—´ç”¨å¦‚ä¸‹åˆ†éš”çº¿åˆ†éš”---------ï¼ˆä¸å¯æ”¹åŠ¨ï¼‰\n"
            "å…¶ä»–è¯´æ˜ï¼š\n"
            "  ä¸è¦è¾“å‡ºè§£é‡Šã€ä»£ç ã€å¼•å·ã€æ ‡è®°è¯­è¨€ï¼›\n"
            "  ä¸è¦åˆ†æç¼ºå¤±é¢˜ï¼›\n"
            "  â€œç†ç”±â€å¿…é¡»å…·ä½“å†™æ˜å·®å¼‚ç‚¹ï¼›\n"
            "è¾“å‡ºå¿…é¡»ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œé€‚åˆå¤åˆ¶ä¿å­˜ä¸º .txt æ–‡ä»¶ã€‚\n\n"
            f"é¢˜å¹²å†…å®¹å¦‚ä¸‹ï¼š\n{question_text}\n\n"
            f"å‚è€ƒåŸå§‹æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{context_text}\n\n"
            f"ç­”æ¡ˆ X-{idx} å†…å®¹å¦‚ä¸‹ï¼š\n{answer_x_text}\n\n"
            f"ç­”æ¡ˆ Y-{idx} å†…å®¹å¦‚ä¸‹ï¼š\n{answer_y_text}"
        )

        
        content = ""
        for attempt in range(1, retries + 1):
            try:
                r = session.post(
                    f"{DEESEEK_API_BASE}/chat/completions",
                    headers={"Authorization": f"Bearer {DEESEEK_API_KEY}", "Content-Type": "application/json"},
                    json={"model": "deepseek-chat", "messages":[{"role":"user","content":prompt}], "temperature":0},
                    timeout=180
                )
                r.raise_for_status()
                content = r.json()["choices"][0]["message"]["content"].strip()
                logger.info(f"compare_qa ç¬¬{idx}é¢˜ç¬¬{attempt}æ¬¡å°è¯•æˆåŠŸ")
                break
            except Exception as e:
                logger.error(f"compare_qa ç¬¬{idx}é¢˜ç¬¬{attempt}æ¬¡å°è¯•å¤±è´¥: {e}")
                time.sleep(5)
        if not content:
            return (
                f"é¢˜å·ï¼š{idx}\n"
                f"é¢˜å¹²ï¼š{question_text}\n"
                f"ç­”æ¡ˆ X-{idx}ï¼š{answer_x_text}\n"
                f"ç­”æ¡ˆ Y-{idx}ï¼š{answer_y_text}\n"
                "ç†ç”±ï¼šè°ƒç”¨ Deepseek API å¤±è´¥ï¼Œæœªèƒ½å®Œæˆå¯¹æ¯”ã€‚\n"
                "ç›¸ä¼¼åº¦ï¼š0\n"
                "è¯„çº§ï¼šä¸åˆæ ¼\n"
                "---------"
            )
        else:
            return content

    results = [None] * len(qa_list)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [
            executor.submit(_compare_one, idx + 1, qa, ans, ctx)
            for idx, (qa, ans, ctx) in enumerate(zip(qa_list, answers, contexts))
        ]
        for fut in as_completed(futures):
            try:
                res = fut.result()
                m = re.search(r"(?:ç­”æ¡ˆ X-|ç­”æ¡ˆ Y-)(\d+)", res)
                if not m:
                    logger.warning(f"compare_qa å†…å®¹æ— æ³•æå–é¢˜å·ï¼š{res[:100]}...")
                    continue
                idx = int(m.group(1)) - 1
                results[idx] = res
            except Exception as e:
                logger.error(f"compare_qa ç»“æœè§£æå¤±è´¥: {e}")
    return results

def ask_agent_sse(questions, retries=3):
    """
    ä½¿ç”¨ SSE æ¨¡å¼å¹¶å‘å‘ Dify æ™ºèƒ½ä½“æé—®ï¼Œè¿”å›æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆåˆ—è¡¨ã€‚
    - questions: é—®é¢˜åˆ—è¡¨
    - retries: æ¯é¢˜æœ€å¤šé‡è¯•æ¬¡æ•°
    """
    answers = [""] * len(questions)

    def _call(idx, q):
        logger.info(f"å‡†å¤‡æé—®ç¬¬ {idx + 1} é¢˜ï¼š{q}")
        text = ""
        for attempt in range(1, retries + 1):
            try:
                r = session.post(
                    CHAT_API_URL,
                    headers={
                        "Authorization": CHAT_API_KEY,
                        "Content-Type": "application/json"
                    },
                    json={
                        "inputs": {},
                        "query": q,
                        "agent_id": AGENT_ID,
                        "response_mode": "streaming",
                        "conversation_id": "",
                        "user": "tester"
                    },

                    stream=True,
                    timeout=180  
                )
                r.raise_for_status()
                last = time.time()

                for line in r.iter_lines():
                    if not line:
                        if time.time() - last > 30:
                            logger.warning(
                                f"SSE è¶…æ—¶ç»ˆæ­¢ï¼šç¬¬ {idx + 1} é¢˜è¶…è¿‡ 30 ç§’æ— å“åº”"
                            )
                            raise TimeoutError("SSE æ— å“åº”è¶…æ—¶")
                        continue

                    last = time.time()
                    ch = line.decode("utf-8", errors="ignore")
                    if not ch.startswith("data:"):
                        continue

                    try:
                        data = json.loads(ch.replace("data:", "", 1).strip())
                        event_type = data.get("event", "")

                        if event_type in ("message", "message_replace"):
                            content_piece = data.get("content") or data.get("answer") or ""
                            if isinstance(content_piece, str) and content_piece.strip():
                                text += content_piece.strip()
                            else:
                                logger.debug(
                                    f"ç¬¬ {idx + 1} é¢˜ç©º messageï¼Œç»§ç»­ç­‰å¾…åç»­æµ: {data}"
                                )

                        elif event_type == "message_end":
                            logger.debug(
                                f"ç¬¬ {idx + 1} é¢˜å·²æ¥æ”¶åˆ°å®Œæ•´å“åº”"
                            )

                        elif event_type == "error":
                            logger.error(
                                f"SSE æœåŠ¡ç«¯è¿”å› error äº‹ä»¶ï¼Œè§¦å‘é‡è¯•: {data}"
                            )
                            raise RuntimeError("SSE æœåŠ¡ç«¯ errorï¼Œè§¦å‘é‡è¯•")

                        else:
                            logger.warning(
                                f"ç¬¬ {idx + 1} é¢˜æ¥æ”¶åˆ°æœªçŸ¥äº‹ä»¶ç±»å‹: {data}"
                            )

                    except Exception as e:
                        logger.error(
                            f"ç¬¬ {idx + 1} é¢˜è§£æ SSE æ•°æ®å¤±è´¥: {ch}, é”™è¯¯: {e}"
                        )

                if text.strip():
                    logger.info(
                        f"ç¬¬ {idx + 1} é¢˜è¿”å›å†…å®¹ï¼ˆå‰100å­—ï¼‰ï¼š{text.strip()[:100]}"
                    )
                    break
                else:
                    logger.warning(
                        f"ç¬¬ {idx + 1} é¢˜å°è¯• {attempt}/{retries} æ— æœ‰æ•ˆè¿”å›å†…å®¹"
                    )
                    time.sleep(2)

            except Exception as e:
                logger.error(
                    f"ç¬¬ {idx + 1} é¢˜ç¬¬ {attempt}/{retries} æ¬¡è¯·æ±‚å¤±è´¥: {e}"
                )
                time.sleep(2)

        else:
            logger.warning(
                f"ç¬¬ {idx + 1} é¢˜æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œæœ€ç»ˆè¿”å›ç©ºå­—ç¬¦ä¸²"
            )

        return idx, text.strip()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        futures = [
            pool.submit(_call, i, q)
            for i, q in enumerate(questions)
        ]
        for fut in as_completed(futures):
            try:
                idx, ans = fut.result() 
                answers[idx] = ans
            except Exception as e:
                logger.error(f"ask_agent_sse å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}")

    for i in range(len(answers)):
        if not answers[i].strip():
            logger.warning(
                f"ç¬¬ {i+1} é¢˜ç­”æ¡ˆç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å¡«å……å€¼å‚ä¸å¯¹æ¯”è¯„åˆ†"
            )
            answers[i] = "ã€ç³»ç»Ÿæç¤ºã€‘ï¼šæœ¬é¢˜æ™ºèƒ½ä½“æœªè¿”å›ç­”æ¡ˆï¼Œæ— æ³•å›ç­”ã€‚"

    return answers




def format_report(qa_list, answers, compares):
    return "\n".join([c for c in compares if isinstance(c, str)])

def main():
    docs = get_documents(DATASET_ID)
    ids, names = parse_file_info(docs)
    for fid, fname in zip(ids, names):
        logger.info(f"å¼€å§‹å¤„ç†ï¼š{fname}")
        segs = get_segments(DATASET_ID, fid)
        texts = [extract_segment_text(s) for s in segs if extract_segment_text(s)]
        if not texts:
            logger.warning(f"{fname} æ— æœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡")
            continue
        content = "\n\n".join(texts)
        qa = generate_qa_pairs_via_llm(content)
        if not qa:
            logger.warning(f"{fname} æ—  QAï¼Œå¯¹æ¯”è·³è¿‡")
            continue
        ans = ask_agent_sse([q["question"] for q in qa])
        contexts = [content] * len(ans)
        cmp_results = compare_qa(qa, ans, contexts)
        report = format_report(qa, ans, cmp_results)
        out_path = os.path.join(REPORT_DIR, sanitize_filename(fname))
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(report)
        logger.info(f"å·²ç”ŸæˆæŠ¥å‘Šï¼š{out_path}")

# ===== æœ¬åœ°åˆå¹¶ TXT åˆ° Excel =====
from openpyxl import load_workbook
from openpyxl.styles import Alignment

import os
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import Alignment, Font

def combine_local(dir_path: str, output_name: str = None):
    filename = output_name or f"{''.join(c if c.isalnum() else '_' for c in KB_NAME)}_combined.xlsx"
    all_data = []
    txt_files = []

    for fname in sorted(os.listdir(dir_path)):
        if fname.endswith(".txt"):
            full_path = os.path.join(dir_path, fname)
            parsed = parse_txt_file(full_path)
            all_data.extend(parsed)
            txt_files.append(full_path)

    if not all_data:
        print("æœªæ‰¾åˆ°ç¬¦åˆæ ¼å¼çš„ txt æ–‡ä»¶")
        return

    df = pd.DataFrame(all_data)
    df = df.drop_duplicates(subset=["æ–‡ä»¶å", "é¢˜å·"], keep="first")

    try:
        df["é¢˜å·"] = df["é¢˜å·"].astype(int)
        df = df.sort_values(["æ–‡ä»¶å", "é¢˜å·"])
    except:
        df = df.sort_values("æ–‡ä»¶å")

    out_path = os.path.join(dir_path, filename)
    df.to_excel(out_path, index=False)

    wb = load_workbook(out_path)
    ws = wb.active

    for row in ws.iter_rows():
        for cell in row:
            cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
            cell.font = Font(name="ç­‰çº¿", size=10)
        ws.row_dimensions[row[0].row].height = 30

    for col in ws.columns:
        max_len = 0
        col_letter = col[0].column_letter
        for cell in col:
            try:
                if cell.value:
                    max_len = max(max_len, len(str(cell.value)))
            except:
                pass
        ws.column_dimensions[col_letter].width = min(max_len * 1.2, 50)  

    wb.save(out_path)
    print(f"âœ… å·²ä¿å­˜åˆå¹¶è¡¨æ ¼åˆ°ï¼š{out_path}")

    deleted_count = 0
    for path in txt_files:
        try:
            os.remove(path)
            deleted_count += 1
        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥ï¼š{path}ï¼ŒåŸå› ï¼š{e}")

    print(f"ğŸ§¹ å·²åˆ é™¤ {deleted_count} ä¸ª .txt æ–‡ä»¶")

from typing import List, Dict
import os
import re

def parse_txt_file(filepath: str) -> List[Dict[str, str]]:
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    blocks = content.split('---------')
    data = []

    for block in blocks:
        block = block.strip()
        if not block:
            continue
        m_qid = re.search(r"ç­”æ¡ˆ\s*[XY]-?(\d+)", block)
        qid = m_qid.group(1).strip() if m_qid else ""

        def extract_field(field_name):
            pattern = rf"(?s){field_name}\s*(?:X|Y)?-?\d*[:ï¼š]\s*([\s\S]*?)(?=\n(?:é¢˜å¹²|ç­”æ¡ˆ X|ç­”æ¡ˆ Y|ç†ç”±|ç›¸ä¼¼åº¦|è¯„çº§)[:ï¼š]|$)"
            m = re.search(pattern, block, flags=re.S)
            return m.group(1).strip() if m else ""

        question    = extract_field("é¢˜å¹²")
        answer_x    = extract_field("ç­”æ¡ˆ X")
        answer_y    = extract_field("ç­”æ¡ˆ Y")
        reason      = extract_field("ç†ç”±")
        similarity  = extract_field("ç›¸ä¼¼åº¦")
        rating      = extract_field("è¯„çº§")

        if not (answer_x or answer_y):
            continue  

        row = {
            "çŸ¥è¯†åº“": KB_NAME,
            "æ–‡ä»¶å": os.path.basename(filepath),
            "é¢˜å·":    qid,
            "é—®ç­”å¯¹":  question,
            "ç†ç”±":    reason,
            "ç›¸ä¼¼åº¦":  similarity,
            "è¯„çº§":    rating,
        }

        data.append(row)

    return data

def process_document(fid, fname):
    logger.info(f"å¼€å§‹å¤„ç†ï¼š{fname}")
    segs = get_segments(DATASET_ID, fid)
    texts = [extract_segment_text(s) for s in segs if extract_segment_text(s)]
    if not texts:
        logger.warning(f"{fname} æ— æœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡")
        return
    content = "\n\n".join(texts)
    qa = generate_qa_pairs_via_llm(content)
    if not qa:
        logger.warning(f"{fname} æ—  QAï¼Œå¯¹æ¯”è·³è¿‡")
        return
    ans = ask_agent_sse([q["question"] for q in qa])
    contexts = [content] * len(ans)  
    cmp_results = compare_qa(qa, ans, contexts) 
    report = format_report(qa, ans, cmp_results)
    out_path = os.path.join(REPORT_DIR, sanitize_filename(fname))
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report)
    logger.info(f"å·²ç”ŸæˆæŠ¥å‘Šï¼š{out_path}")


if __name__ == '__main__':
    docs = get_documents(DATASET_ID)
    ids, names = parse_file_info(docs)
    total_docs = len(ids)
    if total_docs == 0:
        logger.info("æ²¡æœ‰æ–‡æ¡£å¯å¤„ç†ï¼Œç¨‹åºé€€å‡º")
        exit(0)

    num_workers = min(MAX_WORKERS, total_docs)
    logger.info(f"å…± {total_docs} ä¸ªæ–‡æ¡£ï¼Œå°†å¯ç”¨ {num_workers} ä¸ªå¹¶å‘çº¿ç¨‹")

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [
            executor.submit(process_document, fid, fname)
            for fid, fname in zip(ids, names)
        ]
        for fut in as_completed(futures):
            try:
                fut.result()
            except Exception as e:
                logger.error(f"æ–‡æ¡£å¤„ç†æ—¶å‡ºé”™ï¼š{e}")

    combine_local(REPORT_DIR)
